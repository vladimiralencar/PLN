{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Processamento de Linguagem Natural - Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similaridade de Texto\n",
    "\n",
    "Como computar a similaridade entre duas strings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from nltk.cluster.util import cosine_distance\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = 'Refrigerador Brastemp CFR45 20L frostfree'\n",
    "b = 'Geladeira Brastemp CFR45 20L com desgelo automático'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'20L', 'Brastemp', 'CFR45'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokens similares\n",
    "tokensA = a.split()\n",
    "tokensB = b.split()\n",
    "set(tokensA).intersection(tokensB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 tokens similares de 9 tokens: 33.33% de similaridade\n"
     ]
    }
   ],
   "source": [
    "similar = len(set(tokensA).intersection(tokensB))\n",
    "total = len(set(tokensA).union(tokensB))\n",
    "print ('{} tokens similares de {} tokens: {:0.2f}% de similaridade'.format(similar, total, similar/total*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usando word2vec Para Computar Similaridades entre Vetores\n",
    "https://radimrehurek.com/gensim/models/word2vec.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2vec é um grupo de modelos relacionados que são usados para produzir word embeddings. Esses modelos são redes neurais artificiais de duas camadas que são treinadas para reconstruir contextos linguísticos de palavras. O Word2vec toma como entrada um grande corpus de texto e produz um espaço vetorial, tipicamente de várias centenas de dimensões, com cada palavra única no corpus sendo atribuída um vetor correspondente no espaço. Os vetores de palavras são posicionados no espaço vetorial de tal forma que as palavras que compartilham contextos comuns no corpus estão localizadas próximas umas das outras no espaço."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O Word2vec foi criado por uma equipe de pesquisadores liderada por Tomas Mikolov no Google. O algoritmo foi posteriormente analisado e explicado por outros pesquisadores. Incorporar vetores criados usando o algoritmo Word2vec tem muitas vantagens em comparação com algoritmos anteriores como Latent Semantic Analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Leitura do Corpus - Produtos\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Carregando o conteúdo do Corpus para um objeto Python\n",
    "with codecs.open('corpus.txt', encoding = 'utf8') as fp:\n",
    "    corpus = fp.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Kit com 4 Pneus de Alta Performance Pirelli Aro 16 205/55R16 Phantom\\nChegou o kit que junta resistência e conforto, além'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[:120]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tokenization com NLTK - este processo é demorado!!!\n",
    "sentences = [[w.lower() for w in word_tokenize(sentence, language = 'portuguese')] \n",
    "             for sentence in sent_tokenize(corpus, language = 'portuguese')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['kit', 'com', '4', 'pneus', 'de', 'alta', 'performance', 'pirelli', 'aro', '16', '205/55r16', 'phantom', 'chegou', 'o', 'kit', 'que', 'junta', 'resistência', 'e', 'conforto', ',', 'além', 'de', 'níveis', 'máximos', 'de', 'segurança', '.']\n",
      "['são', '4', 'pneus', 'para', 'seu', 'carro', 'ficar', 'completo', 'e', 'com', 'a', 'qualificação', 'que', 'você', 'precisa', '.']\n"
     ]
    }
   ],
   "source": [
    "print(sentences[0])\n",
    "print(sentences[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "?Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Treinando o modelo\n",
    "modelo = Word2Vec(sentences, size = 100, window = 5, min_count = 5, workers = 8)\n",
    "modelo.init_sims(replace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cozinha', 0.603610098361969),\n",
       " ('sala', 0.6010788083076477),\n",
       " ('inverse', 0.5992774963378906),\n",
       " ('bebida', 0.5664677023887634),\n",
       " ('estação', 0.5561043620109558),\n",
       " ('garagem', 0.5558688640594482),\n",
       " ('torneira', 0.5476281046867371),\n",
       " ('banheira', 0.5454915761947632),\n",
       " ('facilite', 0.5445488691329956),\n",
       " ('condensadora', 0.529249370098114)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelo.most_similar('geladeira')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dispositivo', 0.7884957194328308),\n",
       " ('telefone', 0.787076473236084),\n",
       " ('pc', 0.7727987170219421),\n",
       " ('navegador', 0.7233446836471558),\n",
       " ('computador-', 0.7225916981697083),\n",
       " ('veículo', 0.6774432063102722),\n",
       " ('aplicativo', 0.6563814878463745),\n",
       " ('console', 0.656001091003418),\n",
       " ('conecte', 0.6487194895744324),\n",
       " ('pendrive', 0.635190486907959)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelo.most_similar('computador')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('veicular', 0.8345345854759216),\n",
       " ('adaptador', 0.8042702674865723),\n",
       " ('12v', 0.7721058130264282),\n",
       " ('recarregável', 0.7611072659492493),\n",
       " ('bivolt', 0.7256201505661011),\n",
       " ('bastão', 0.7100176811218262),\n",
       " ('lítio', 0.7043484449386597),\n",
       " ('dock', 0.7039474844932556),\n",
       " ('usb3.0', 0.696510374546051),\n",
       " ('rj45', 0.6964977383613586)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelo.most_similar('carregador')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Brown - 1.014.312 words\n",
    "Political\n",
    "Sports\n",
    "Society\n",
    "Spot News\n",
    "Financial\n",
    "Cultural, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from nltk.corpus import brown\n",
    "model = gensim.models.Word2Vec(brown.sents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.84210825691298152"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similarity('university','school')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cereal'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doesnt_match('breakfast cereal dinner lunch'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('clothes', 0.8275762796401978)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['woman','home'], negative=['man'], topn = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Spain', 0.937682032585144)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['Paris','Germany'], negative=['Berlin'], topn = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('specific', 0.9589784145355225),\n",
       " ('adjustment', 0.9588124752044678),\n",
       " ('substantial', 0.9587265253067017),\n",
       " ('actual', 0.9569110870361328),\n",
       " ('substance', 0.9552959203720093),\n",
       " ('wage', 0.9545634984970093),\n",
       " ('provision', 0.9526653289794922),\n",
       " ('requirement', 0.9524614810943604),\n",
       " ('analysis', 0.952355146408081),\n",
       " ('primarily', 0.9522107839584351)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('care', 0.9107005596160889),\n",
       " ('chance', 0.8993101716041565),\n",
       " ('job', 0.8890410661697388),\n",
       " ('everything', 0.863770067691803),\n",
       " ('trouble', 0.8619292378425598)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('money', topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset subjectivity - temas variados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import subjectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(subjectivity.sents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99844252724117244"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similarity('university','school')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('boy', 0.9965816736221313)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['woman', \"work\"], negative=['age'], topn = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('fall', 0.8365045785903931),\n",
       " ('so', 0.8174225091934204),\n",
       " ('forum', 0.8065845966339111),\n",
       " ('her', 0.7995058298110962),\n",
       " ('yours', 0.7960962057113647),\n",
       " ('est', 0.7954405546188354),\n",
       " ('christina', 0.7952874898910522),\n",
       " ('gang', 0.7915248870849609),\n",
       " ('calvin', 0.7889474630355835),\n",
       " ('let', 0.7771402597427368)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelo.most_similar('man')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('provides', 0.906433641910553),\n",
       " ('offers', 0.8989260196685791),\n",
       " ('approach', 0.8901066780090332),\n",
       " ('gives', 0.8850250244140625),\n",
       " ('through', 0.8822256326675415),\n",
       " ('read', 0.8804838061332703),\n",
       " ('comprehensive', 0.8787658214569092),\n",
       " ('starting', 0.8769986629486084),\n",
       " ('presents', 0.8768618702888489),\n",
       " ('great', 0.8727860450744629)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelo.most_similar('book')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Dataset MovieReviews - Cinema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import movie_reviews, treebank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['plot', ':', 'two', 'teen', 'couples', 'go', 'to', 'a', 'church', 'party', ',', 'drink', 'and', 'then', 'drive', '.'], ['they', 'get', 'into', 'an', 'accident', '.'], ...]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_reviews.sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['plot', ':', 'two', 'teen', 'couples', 'go', 'to', 'a', 'church', 'party', ',', 'drink', 'and', 'then', 'drive', '.', 'they', 'get', 'into', 'an', 'accident', '.', 'one', 'of', 'the', 'guys', 'dies', ',', 'but', 'his']\n"
     ]
    }
   ],
   "source": [
    "print(movie_reviews.words()[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# aplica o algoritmo Word2Vec\n",
    "mr = Word2Vec(movie_reviews.sents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('princess', 0.8578184843063354),\n",
       " ('robin', 0.8438497185707092),\n",
       " ('captain', 0.8431352376937866),\n",
       " ('neil', 0.8429046869277954),\n",
       " ('edward', 0.8417447805404663),\n",
       " ('chris', 0.8391830325126648),\n",
       " ('george', 0.8385847806930542),\n",
       " ('david', 0.8332217931747437),\n",
       " ('paul', 0.8303455114364624),\n",
       " ('jackson', 0.8275794982910156)]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mr.most_similar('king')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('mother', 0.9693006277084351),\n",
       " ('wife', 0.9567008018493652),\n",
       " ('son', 0.9455345869064331),\n",
       " ('brother', 0.9371278882026672),\n",
       " ('daughter', 0.9293205738067627),\n",
       " ('husband', 0.9215914011001587),\n",
       " ('girlfriend', 0.9062857627868652),\n",
       " ('sister', 0.8998033404350281),\n",
       " ('boyfriend', 0.8947833776473999),\n",
       " ('boss', 0.8794924020767212)]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mr.most_similar('father')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('good', 0.8460168838500977),\n",
       " ('cool', 0.7931435108184814),\n",
       " ('stupid', 0.7368031740188599),\n",
       " ('funny', 0.7174288034439087),\n",
       " ('nice', 0.7068173885345459),\n",
       " ('dumb', 0.7039850354194641),\n",
       " ('great', 0.6515941619873047),\n",
       " ('scary', 0.643081545829773),\n",
       " ('tough', 0.6386542916297913),\n",
       " ('basically', 0.63398277759552)]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mr.most_similar('bad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('slasher', 0.8947128653526306),\n",
       " ('science', 0.865003228187561),\n",
       " ('fiction', 0.8451724052429199),\n",
       " ('animated', 0.8113899827003479),\n",
       " ('action', 0.7863391637802124),\n",
       " ('disaster', 0.7821124792098999),\n",
       " ('teen', 0.7691265344619751),\n",
       " ('genre', 0.7654432058334351),\n",
       " ('budget', 0.7644156217575073),\n",
       " ('pulp', 0.7450346946716309)]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mr.most_similar('horror')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('planet', 0.7939170598983765),\n",
       " ('york', 0.7932645082473755),\n",
       " ('space', 0.7749285697937012),\n",
       " ('south', 0.7659956812858582),\n",
       " ('world', 0.7627872228622437),\n",
       " ('apes', 0.7576571702957153),\n",
       " ('angels', 0.7515949010848999),\n",
       " ('united', 0.7486950755119324),\n",
       " ('war', 0.7461063861846924),\n",
       " ('jungle', 0.7420922517776489)]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mr.most_similar('city')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('caricatures', 0.895542323589325),\n",
       " ('cardboard', 0.8932735919952393),\n",
       " ('standouts', 0.891860842704773),\n",
       " ('barrels', 0.8906723260879517),\n",
       " ('musicians', 0.8880579471588135),\n",
       " ('passengers', 0.8830095529556274),\n",
       " ('males', 0.878976047039032),\n",
       " ('oppression', 0.8779348134994507),\n",
       " ('artists', 0.8778836727142334),\n",
       " ('painted', 0.8760911226272583)]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mr.most_similar('couples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset brasileiro - Floresta (textos variados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Um', 'revivalismo', 'refrescante'], ['O', '7_e_Meio', 'é', 'um', 'ex-libris', 'de', 'a', 'noite', 'algarvia', '.'], ...]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import floresta\n",
    "floresta.sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = Word2Vec(floresta.sents(), size = 100, window = 5, min_count = 5, workers = 8)\n",
    "f.init_sims(replace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('passou', 0.9998331069946289),\n",
       " ('meio', 0.9997837543487549),\n",
       " ('Folha', 0.9997823238372803),\n",
       " ('outras', 0.9997642636299133),\n",
       " ('hora', 0.9997502565383911),\n",
       " ('partida', 0.9997397661209106),\n",
       " ('rede', 0.9997308254241943),\n",
       " ('jogadores', 0.9997305870056152),\n",
       " ('parar', 0.999725341796875),\n",
       " ('reunião', 0.9996980428695679)]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.most_similar('casa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('história', 0.9997707605361938),\n",
       " ('caso', 0.9997049570083618),\n",
       " ('morte', 0.9996691346168518),\n",
       " ('está', 0.9996117353439331),\n",
       " ('já', 0.9995875358581543),\n",
       " ('vai', 0.9995788931846619),\n",
       " ('rua', 0.9995462894439697),\n",
       " ('também', 0.9995440244674683),\n",
       " ('depois', 0.9995191097259521)]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.most_similar('política', topn=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('parte', 0.9997426271438599),\n",
       " ('já', 0.9996792674064636),\n",
       " ('através', 0.9996192455291748),\n",
       " ('Europa', 0.9996017217636108),\n",
       " ('depois', 0.999599277973175),\n",
       " ('Segundo', 0.9995843768119812),\n",
       " ('bem_como', 0.9995585680007935),\n",
       " ('mais', 0.9995459914207458),\n",
       " ('último', 0.9995173215866089),\n",
       " ('abertura', 0.9994987845420837),\n",
       " ('morte', 0.9994944930076599),\n",
       " ('mesma', 0.999457836151123),\n",
       " ('primeira', 0.9994251728057861),\n",
       " ('produção', 0.9994106292724609),\n",
       " ('este', 0.9994090795516968),\n",
       " ('cinema', 0.9994083046913147),\n",
       " ('até', 0.9993873834609985),\n",
       " ('fim', 0.999384880065918),\n",
       " ('trabalho', 0.9993780255317688),\n",
       " ('política', 0.9993778467178345)]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.most_similar('noite', topn=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
